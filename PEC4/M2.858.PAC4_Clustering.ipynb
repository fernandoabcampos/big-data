{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n##PEC4: Segmentación comportamental basada en clustering (ML no supervisado) \n\nEsta PEC simula el ejercicio completo de generación de segmentos (clusters) de clientes utilizando un dataset de usuarios y evaluaciones de películas liberados por una empresa de streaming multimedia bajo demanda por Internet.\n\n**Este notebook cubre:**\n* *Parte 1:* ETL de los ficheros (1 puntos)\n* *Parte 2:* Ingeniería de características (2 puntos)\n* *Parte 3:* Clustering (2 puntos)\n* *Parte 4:* Validación de los resultados (2.5 puntos)\n* *Parte 5:* Normalización de los datos (2.5 puntos)\n\n**IMPORTANTE:** Versión de spark para realizar esta PAC: 2.3"],"metadata":{}},{"cell_type":"markdown","source":["## Parte 1: ETL de los ficheros\n\n** Resumen **\n\nLos archivos que usaremos contienen 1.000.209 calificaciones anónimas de aproximadamente 3.900 películas de 6.040 usuarios realizadas durante el año 2000.\n\n** Fichero de calificaciones **\n\nTodas las calificaciones están contenidas en el archivo \"ratings.dat\" y tienen el siguiente formato:\n\n*UserID::MovieID::Rating::Timestamp*\n\n- UserIDs: cuyo rango se encuentra entre 1 y 6040\n- MovieIDs: cuyo rango se encuentra entre 1 y 3952\n- Rating: las calificaciones se realizan en una escala de 5 estrellas (sin decimales)\n- Timestamp: la marca de tiempo se representa en segundos\n\nNota: Para cada usuario hay como mínimo 20 calificaciones\n\n** Fichero de usuarios **\n\nLa información sobre los usuarios está contenida en el fichero \"users.dat\" y tiene el siguiente formato:\n\n*UserID::Gender::Age::Occupation::Zip-code*\n\nToda la información demográfica es proporcionada voluntariamente por los usuarios y no se comprueba su exactitud. Solo los usuarios que hayan proporcionado datos demográficos se incluyen en este conjunto de datos.\n\n- Gender: El género se denota por \"M\" para hombres y \"F\" para mujeres\n- Age: Distibuida en los siguientes rangos:\n\n\t*  1:  \"- 18\"\n\t* 18:  \"18-24\"\n\t* 25:  \"25-34\"\n\t* 35:  \"35-44\"\n\t* 45:  \"45-49\"\n\t* 50:  \"50-55\"\n\t* 56:  \"56 - \"\n\n- Occupation: Se elige de las siguientes opciones:\n\n\t*  0:  \"other\" or not specified\n\t*  1:  \"academic/educator\"\n\t*  2:  \"artist\"\n\t*  3:  \"clerical/admin\"\n\t*  4:  \"college/grad student\"\n\t*  5:  \"customer service\"\n\t*  6:  \"doctor/health care\"\n\t*  7:  \"executive/managerial\"\n\t*  8:  \"farmer\"\n\t*  9:  \"homemaker\"\n\t* 10:  \"K-12 student\"\n\t* 11:  \"lawyer\"\n\t* 12:  \"programmer\"\n\t* 13:  \"retired\"\n\t* 14:  \"sales/marketing\"\n\t* 15:  \"scientist\"\n\t* 16:  \"self-employed\"\n\t* 17:  \"technician/engineer\"\n\t* 18:  \"tradesman/craftsman\"\n\t* 19:  \"unemployed\"\n\t* 20:  \"writer\"\n    \n** Fichero de películas **\n\nLa información se ecuentra en el archivo \"movies.dat\" y está en el siguiente\nformato:\n\n*MovieID::Title::Genres*\n\n- Title: Los títulos son idénticos a los títulos proporcionados por la IMDB (incluyendo año de lanzamiento)\n- Genres: Los géneros están separados por *pipes* y se seleccionan de la siguiente lista:\n    * Action\n\t* Adventure\n\t* Animation\n\t* Children's\n\t* Comedy\n\t* Crime\n\t* Documentary\n\t* Drama\n\t* Fantasy\n\t* Film-Noir\n\t* Horror\n\t* Musical\n\t* Mystery\n\t* Romance\n\t* Sci-Fi\n\t* Thriller\n\t* War\n\t* Western\n    \n- Algunos MovieIDs no corresponden a una pelicula debido a un duplicado accidental a su entradas y/o a entradas de prueba\n- Las películas se introducen principalmente a mano, por lo que pueden existir errores e incoherencias"],"metadata":{}},{"cell_type":"markdown","source":["### Ejercicio 1(a):\n\nEmpezaremos por visualizar una muestra de los datos. Para esto usaremos las funciones pre-definidas en los notebooks de Databricks para explorar su sistema de archivos.\n\nUsar `display(dbutils.fs.ls(\"/databricks-datasets/cs110x/ml-1m/data-001\")` para mostrar la descripción de los datos que se van a usar."],"metadata":{}},{"cell_type":"code","source":["#TODO: use display to list all the files of the directory containing the data\n<FILL IN>\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Ahora usaremos el comando `print dbutils.fs.head` para visualizar el contenido de los ficheros \"movies.dat\", \"user.dat\" y \"ratings.dat\""],"metadata":{}},{"cell_type":"code","source":["#TODO: use dbutils.fs.head to inspect the file \"user.dat\"\n<FILL IN>"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#TODO: use dbutils.fs.head to inspect the file \"ratings.dat\"\n<FILL IN>"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#TODO: use dbutils.fs.head to inspect the file \"movies.dat\"\n\n<FILL IN>"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Ejercicio 1(b):\n\nCargar los diferentes ficheros (usuarios, películas y calificaciones) en tres RDDs para su posterior procesado."],"metadata":{}},{"cell_type":"code","source":["# TODO: Load the users data and print the first five lines.\n\nrawUsersTextRdd = <FILL IN>\nprint <FILL IN>"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# TODO: Load the ratings data and print the first five lines.\n\nrawRatingsTextRdd = <FILL IN>\nprint <FILL IN>"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# TODO: Load the movies data and print the first five lines.\n\nrawMoviesTextRdd = <FILL IN>\nprint <FILL IN>"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Ejercicio 1(c): \n\nComo has observado inspecionando los ficheros, el fichero de usuarios contiene algunas líneas con más de un zip-code. Para simplicidad, eliminaremos estas líneas del fichero. Para eso crearemos una función que elimine todas las líneas que contenga algún caracter que no sea '0123456789:MF'."],"metadata":{}},{"cell_type":"code","source":["#TODO: Complete the function invalidLine\ndef invalidLine(line):\n    \"\"\"Verifies if a line is valid to be converted to a dataframe.\n    Args:\n        line (str): A string.\n\n    Returns:\n        boolean: True if valid, False otherwise.\n    \"\"\"\n  <FILL IN>"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Load in the testing code and check to see if your answer is correct\n# If incorrect it will report back '1 test failed' for each failed test\n# Make sure to rerun any cell you change before trying the test again\nfrom databricks_test_helper import Test\n# TEST invalidLine (1b)\nTest.assertEquals(invalidLine('161::M::45::16::98107-2117'), False, 'incorrect result: invalidLine does not remove the incorrect lines')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Ejercicio 1(d)\n\nComo en anteriores ocasiones, crea un esquema a medida para cada uno de los ficheros."],"metadata":{}},{"cell_type":"code","source":["#TODO: Fill in the user schema.\nfrom pyspark.sql.types import *\n\n# Custom Schema for users\nuserSchema = StructType([ \\\n    <FILL IN>\n                          ])"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Custom Schema for ratings\nratingsSchema = StructType([ \\\n    <FILL IN>\n                           ])"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Custom Schema for movies\nmoviesSchema = StructType([ \\\n    <FILL IN>\n                          ])"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Ejercicio 1(e)\n\nElimina las líneas no válidas contenidas en `rawUsersTextRdd`."],"metadata":{}},{"cell_type":"code","source":["#TODO: filter invalid lines\nfilteredUsersTextRdd = <FILL IN>"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# TEST filter invalidLines (1d)\ncounter = filteredUsersTextRdd.count()\nTest.assertEquals(counter, 5974, 'incorrect result: lines are incorrectly filtered')"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Ejercicio 1(f)\n\nPara poder usar fácilmente diferentes algoritmos de clustering es conveniente que todos los atributos sean numéricos, para esto convertiremos el atributo gender de los usuarios en 1 si es hombre ('M') o 0 si es mujer ('F'), cambia en el esquema que has creado anteriormente el atributo gender a `IntegerType()` si lo tenías como `char` o `string`.\n\nLuego transforma el RDD en un DataFrame usando la función [toDF()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext) indicándole que schema debe usar como un parámetro de la función (`schema= customSchema`).\n\nPara realizar esta transformación a DataFrame, primero hay que convertir cada línea en una lista de enteros, ya que así se indica en nuestro esquema. Para esto usaremos las funciones de Python [split()](https://docs.python.org/2/library/stdtypes.html#str.split) y [int()](https://docs.python.org/2/library/functions.html#int). Puedes combinar estas dos transformaciones junto con la conversión del atributo `gender` en la misma función lambda o hacer una función con nombre. Luego usa una función `map` para aplicar los cambios a todas las lineas del RDD de usuarios.\n\nFinalmente, comprueba que los datos mostrados por el comando `display(usersDF)` son correctos."],"metadata":{}},{"cell_type":"code","source":["#TODO: transform the Gender type and create a DataFrame\nintegerGendersUsersTextRdd = filteredUsersTextRdd.<FILL IN>\nusersDF = <FILL IN>\n\ndisplay(usersDF)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Ejercicio 1(g)\n\nAhora aplica las mismas transformaciones, excepto la del atributo gender, al RDD de ratings."],"metadata":{}},{"cell_type":"code","source":["#TODO: Create a DataFrame for ratings RDD\nratingsDF = <FILL IN>\n\ndisplay(ratingsDF)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Parte 2: Ingeniería de características\n\nLa ingeniería de características es el proceso de utilizar el conocimiento del dominio de los datos para crear las características que hacen que los algoritmos de machine learning trabajen de forma correcta. Esto es fundamental en la aplicación del machine learning a datos del mundo real, y en general es difícil y costosa. La ingeniería de características es un tema informal, pero se considera esencial en el machine learning aplicado."],"metadata":{}},{"cell_type":"markdown","source":["### Ejercicio 2(a)\n\nPara esta PAC calcularemos unas cuantas características muy sencillas como el número de películas vistas por cada usuario, la media de sus calificaciones y su varianza.\n\nPara obtener estas características usaremos la función [groupBy()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) de los DataFrames de Spark, así como las funciones de agregación que nos proporciona SparkQL (https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#module-pyspark.sql.functions).\n\nPor simplicidad usaremos la función [alias()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext) para poner los siguientes nombres a las columnas: NumMovies, AvgRating y VarRating."],"metadata":{}},{"cell_type":"code","source":["#TODO: Compute aggregated values by user using groupBy and SQL functions\nimport pyspark.sql.functions as func\n\naggRatingsDF = <FILL IN>\n\ndisplay(aggRatingsDF)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# TEST compute aggregated features (2a)\ncounter = aggRatingsDF.count()\nresult = aggRatingsDF.where(aggRatingsDF.UserID == 31).first()\nTest.assertEquals(counter, 6040, 'incorrect result: aggregation is incorreclty done')\nTest.assertEquals(result[\"NumMovies\"], 119, 'incorrect result: NumMovies is incorrect')\nTest.assertEquals(result[\"AvgRating\"], 3.73109243697479, 'incorrect result: AvgRating is incorrect')\nTest.assertEquals(result[\"VarRating\"], 1.0457199829084174, 'incorrect result: NumMovies is incorrect')\nprint result"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["###Ejercicio 2(b)\n\nAhora juntaremos las características que hemos extraido del fichero de ratings con el fichero de usuarios. Como no queremos tener que preocuparnos por los valores nulos en las características, solo nos quedaremos con los usuarios que hayan calificado alguna película. Para realizar esto, usaremos un inner join. Encontraremos los detalles de esta función aquí [join()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html)."],"metadata":{}},{"cell_type":"code","source":["#TODO: Compute an inner join between usersDF and aggRatingsDF\njoinedDF = <FILL IN>\ndisplay(joinedDF)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# TEST join (2b)\ncounter = joinedDF.count()\nTest.assertEquals(counter, 5974, 'incorrect result: join is incorreclty done')"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### Ejercicio 2(c)\n\nAhora vamos a guardar los datos generados para poder reutilizarlos en un futuro sin tener que re-ejecutar todo el notebook, para esto ejecutaremos el siguiente código."],"metadata":{}},{"cell_type":"code","source":["#TODO: Read, understand and execute the cell\nsqlContext.sql(\"DROP TABLE IF EXISTS joinedDF\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/joinedDF\", True)\nsqlContext.registerDataFrameAsTable(joinedDF, \"joinedDF\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## Parte 3: Clustering\n\nUn algoritmo de agrupamiento (en inglés, clustering) es un procedimiento de agrupación de una serie de vectores de acuerdo con un criterio. Esos criterios son por lo general distancia o similitud. La cercanía se define en términos de una determinada función de distancia, como la de Euclides, aunque existen otras mas robustas o que permiten extenderla a variables discretas.\n\nExisten dos grandes familias de clustering:\n\n* *Agrupamiento jerárquico*, que puede ser aglomerativo o divisivo.\n* *Agrupamiento no jerárquico*, en los que el número de grupos se determina de antemano y las observaciones se van asignando a los grupos en función de su cercanía. En esta familia existen una gran cantidad de métodos, en esta PAC usaremos el método de k-means (k-medias).\n\n### k-means\n\nK-means es un método de clustering, que tiene como objetivo la partición de un conjunto de _n_ observaciones en _k_ grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano. Una de las ventajas de este método es que la agrupación del conjunto de datos puede ilustrarse en una partición del espacio de datos en [celdas de Voronoi](https://es.wikipedia.org/wiki/Pol%C3%ADgonos_de_Thiessen#Diagramas_de_Voron.C3.B3i_en_el_plano_euclidiano_.7F.27.22.60UNIQ--postMath-00000001-QINU.60.22.27.7F).\n\nEl problema es computacionalmente difícil (NP-hard). Sin embargo, hay heurísticas muy eficientes que se emplean comunmente y que convergen rápidamente a un óptimo local.\n\nEl algoritmo de k-means más común utiliza una técnica de refinamiento iterativo, tal y como se describe a continuación:\n\nDado un conjunto inicial de _k_ centroides $$ m_1^{(1)},...,m_k^{(1)} $$ el algoritmo continua alternando entre estos dos pasos:\n\n* *Paso de asignación:* Asigna cada observación al grupo con la media más cercana (es decir, la partición de las observaciones de acuerdo con el diagrama de Voronoi generado por los centroides).\n\n$$ S_{i}^{(t)} = \\\\{ x_p: || x_p - m_i^{(t)} || \\leq || x_p - m_j^{(t)} || \\forall 1 \\leq j \\leq k \\\\} $$\n\n* *Paso de actualización:* Calcular los nuevos centroides como el centroide de las observaciones en el grupo.\n$$ m_i^{(t+1)} = \\frac{1}{|S_i^{(t)}|} \\sum^{x_j \\in S_i^{(t)}} x_j$$\n\nEl algoritmo se considera que ha convergido cuando las asignaciones ya no cambian. Los centroides suelen iniciarse de forma aleatoria."],"metadata":{}},{"cell_type":"markdown","source":["El siguiente paso es preparar los datos para aplicar el k-means. Dado que todo el dataset es numérico y consistente, esta será una tarea sencilla y directa.\n\nEl objetivo es utilizar el método de clustering para determinar _k_ usuarios estándar (promedio) que representen la totalidad de los usuarios que tenemos en nuestro dataset. El primer paso en la construcción de nuestro modelo de clustering es convertir las características que hemos calculado en nuestro DataFrame a un vector de características utilizando el método [pyspark.ml.feature.VectorAssembler()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler).\n\nEl VectorAssembler es una transformación que combina una lista dada de columnas en una único vector. Esta transformación es muy útil cuando queremos combinar características en crudo de los datos con otras generadas al aplicar diferentes funciones sobre los datos en un único vector de características. Para integrar en un único vector toda esta información antes de ejecutar un algoritmo de aprendizaje automático, el VectorAssembler toma una lista con los nombres de las columnas de entrada (lista de strings) y el nombre de la columna de salida (string).\n\n### Ejercicio 3(a)\n\n- leer la documentación y los ejemplos de uso de [VectorAssembler](https://spark.apache.org/docs/1.6.2/ml-features.html#vectorassembler)\n- Convertir la tabla SQL `joinedDF` en un `dataframe` llamado datasetDF usando la función table del sqlContext\n- Establecer las columnas de entrada del VectorAssember: `['Age','NumMovies','AvgRating','VarRating']`\n- Establecer la columnas de salida como `\"features\"`"],"metadata":{}},{"cell_type":"code","source":["#TODO: Replace <FILL_IN> with the appropriate code\nfrom pyspark.ml.feature import VectorAssembler\n\ndatasetDF = <FILL IN>\n\nvectorizer = VectorAssembler()\nvectorizer.setInputCols(<FILL IN>)\nvectorizer.setOutputCol(<FILL IN>)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### Ejercicio 3(b)\n\nGuardar en la memoria cache el dataframe `datasetDF` y renombrarlo a clusteringDF"],"metadata":{}},{"cell_type":"code","source":["#TODO: Let's cache the dataset for performance\nclusteringDF = <FILL IN>"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["### Ejercicio 3(c)\n\n- Leer la documentación y los ejemplos de [k-means](https://spark.apache.org/docs/1.6.2/ml-clustering.html#k-means)\n- Ejecutar la siguiente celda"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\nfrom pyspark.ml import Pipeline\n\nkmeans = KMeans()\nprint(kmeans.explainParams())"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["La siguiente celda está basada en [Spark 2.0.1 ML Pipeline API for clustering](https://spark.apache.org/docs/2.0.1/mllib-clustering.html#k-means).\n\nEl primer paso es establecer los valores de los parámetros:\n- Definir el número de clusters (k) como 10\n- Definir el número máximo de iteraciones a 25\n- Definir el random seed a 1\n\nAhora, crearemos el [ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Pipeline) (flujo de ejecución) y estableceremos las fases del pipeline como vectorizar y posteriormente aplicar el algoritmo de clustering que hemos definido.\n\nFinalmente, crearemos el modelo entrenándolo con el DataFrame `clusteringDF`.\n\n### Ejercicio 3(d)\n\n- Leer la documentación [k-means](https://spark.apache.org/docs/2.0.1/mllib-clustering.html#k-means).\n- Completa y ejecuta la siguiente celda, asegúrate de entender que es lo que sucede."],"metadata":{}},{"cell_type":"code","source":["## TODO: Replace <FILL_IN> with the appropriate code\n# Now we set the parameters for the method\nkmeans = KMeans().setK(<FILL IN>).setSeed(<FILL IN>)\n\nclusteringPipeline = Pipeline()\n\n# We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\nclusteringPipeline.setStages(<FILL IN>)\n\n# Let's train on the entire dataset to see what we get\nclusteringModel = clusteringPipeline.fit(<FILL IN>)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["### Ejercicio 3(e)\n\nEjecutar la siguiente celda y observar los centroides que has obtenido. Luego responde a las siguientes preguntas:\n\n- ¿Crees que los centrodides son significativos?\n- ¿Qué crees que ha sucedido?"],"metadata":{}},{"cell_type":"code","source":["# Shows the result.\ncenters = clusteringModel.stages[1].clusterCenters()\nprint(\"Centroids: \")\nfor center in centers:\n    print(center)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["## Parte 4: Validación de los resultados\n\nAhora estudiaremos como se comportan nuestras predicciones con este modelo.\n\nPara realizar esta validación, utilizaremos el [Error cuadrático medio](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE).\n\nRecordar que el RSME se define como: \\\\( RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}\\\\) donde \\\\(y_i\\\\) es el valor observado \\\\(x_i\\\\) es el valor predicho\n\nRMSE es una medida muy habitual en los métodos de clustering para calcular las diferencias entre los valores predichos (centroides) y los elementos que componen ese cluster. Cuanto menor sea el RMSE, mejor será nuestro método de clustering."],"metadata":{}},{"cell_type":"markdown","source":["### Ejercicio 4(a)\n\nEl modelo de `k-means` de Spark proporciona un método para calcular la suma de las distancias al cuadrado de todos los elementos de un DataFrame a su centroide más cercano. Esta medida se conoce como SSE (_Sum of Square Errors_). A partir de este valor podemos a calcular el RMSE."],"metadata":{}},{"cell_type":"code","source":["## TODO: Replace <FILL_IN> with the appropriate code\n\nkmeansModel = clusteringModel.stages[1]\ntransformedDF = clusteringModel.stages[0].transform(clusteringDF)\n\n#Obtain the SSE\nSSE = kmeansModel.computeCost(<FILL IN>)\n\n#obtain the number of elements of the DataFrame clusteringDF\nn = <FILL IN>\n\n#compute RMSE\nRMSE = <FILL IN>"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["#TEST RMSE 4(a)\nTest.assertEquals(n, 5974, 'incorrect result: number of elements is wrong')\nTest.assertEquals(round(RMSE,3), 29.437, 'incorrect result: RMSE error')\n"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["### Ejercicio 4(b)\n\nInspecciona visualmente si los centroides predichos de algunos de los elementos de clusteringDF están cerca de los valores reales y si tienen sentido, luego responde las siguientes preguntas:\n\n- ¿Como afecta el hecho de no haber normalizado los datos? \n- ¿Hay algún atributo más importante que los demás? En caso que la respuesta sea afirmativa, ¿Cual?\n\nEjecuta la siguiente celda para ver en que cluster se han asignado cada uno de los registros del DataFrame. Los identificadores de cluster empiezan por 0."],"metadata":{}},{"cell_type":"code","source":["display(kmeansModel.transform(transformedDF).select(['Age','NumMovies','AvgRating','VarRating','prediction']))"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["## Parte 5: Normalización de los datos\n\nUna buena practica cuando usamos métodos de machine learning basados en distancias es asegurarnos que todos los atributos están en la misma escala. Esto no sucede en nuestro dataset ya que cada atributo tiene un rango diferente.\n\nPrueba las siguientes opciones de normalización y re-ejecuta el notebook con los datos normalizados:\n\n* **normalización:** Convierte todos los atributos al rango [0,1] usando la siguiente fórmula\n$$ x' = \\frac{x - min}{max-min} $$\n\n* **estandarización (z-scores):** Convierte todos los atributos en una distribución normal con media = 0 y varianza = 1 usando la siguiente fórmula\n$$ z = \\frac{x - \\mu}{\\sigma}$$\n\n- ¿Tienen ahora los centroides más sentido?\n- ¿Has conseguido reducir error? ¿Esto implica que ahora los centroides están mejor calculados?"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":54}],"metadata":{"name":"PEC3_Clustering_solved","notebookId":780826597561098},"nbformat":4,"nbformat_minor":0}
