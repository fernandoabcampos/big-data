{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n## PEC3: Noviembre 2018\n#Practica sobre cómo generar un flujo de ejecución en un problema de Machine Learning\n## UOC - Máster en Data Science\n### Alumno: **Fernando Antonio Barbeiro Campos** - fbarbeiro@uoc.edu\n\nEsta práctica simula un ejercicio completo de ETL (Extract-Transform-Load) junto a un análisis exploratorio de un dataset real, para posteriormente aplicar differentes algoritmos de aprendizaje automático que resuelvan un problema de regresión.\n\n** This notebook covers: **\n* *Parte 1: Conocimiento del dominio*\n* *Parte 2: Extracción, transformación y carga [ETL] del dataset* (1 punto sobre 10)\n* *Parte 3: Explorar los datos* (1 puntos sobre 10)\n* *Parte 4: Visualizar los datos* (1 puntos sobre 10)\n* *Parte 5: Preparar los datos* (1 puntos sobre 10)\n* *Parte 6: Modelar los datos* (2 puntos sobre 10)\n* *Parte 7: Ajustar y evaluar* (2 puntos sobre 10)\n* *Parte 8: Propuesta de modelo altenativo* (2 puntos sobre 10)\n\n*Nuestro objetivo será predecir de la forma más exacta posible la energía generada por un conjunto de plantas eléctricas usando los datos generados por un conjunto de sensores.*\n\n\n## Parte 1: Conocimiento del dominio\n\n** Background **\n\nLa generación de energía es un proceso complejo, comprenderlo para poder predecir la potencia de salida es un elemento vital en la gestión de una planta energética y su conexión a la red. Los operadores de una red eléctrica regional crean predicciones de la demanda de energía en base a la información histórica y los factores ambientales (por ejemplo, la temperatura). Luego comparan las predicciones con los recursos disponibles (por ejemplo, plantas, carbón, gas natural, nuclear, solar, eólica, hidráulica, etc). Las tecnologías de generación de energía, como la solar o la eólica, dependen en gran medida de las condiciones ambientales, pero todas las centrales eléctricas son objeto de mantenimientos tanto planificados y como puntuales debidos a un problema.\n\nEn esta practica usaremos un ejemplo del mundo real sobre la demanda prevista (en dos escalas de tiempo), la demanda real, y los recursos disponibles de la red electrica de California: http://www.caiso.com/Pages/TodaysOutlook.aspx\n\n![](http://content.caiso.com/outlook/SP/ems_small.gif)\n\nEl reto para un operador de red de energía es cómo manejar un déficit de recursos disponibles frente a la demanda real. Hay tres posibles soluciones a un déficit de energía: construir más plantas de energía base (este proceso puede costar muchos anos de planificación y construcción), comprar e importar de otras redes eléctricas regionales energía sobrante (esta opción puede ser muy cara y está limitado por las interconexiones entre las redes de transmisión de energía y el exceso de potencia disponible de otras redes), o activar pequeñas [plantas de pico](https://en.wikipedia.org/wiki/Peaking_power_plant). Debido a que los operadores de red necesitan responder con rapidez a un déficit de energía para evitar un corte del suministro, estos basan sus decisiones en una combinación de las dos últimas opciones. En esta práctica, nos centraremos en la última elección.\n\n** La lógica de negocio **\n\nDebido a que la demanda de energía solo supera a la oferta ocasionalmente, la potencia suministrada por una planta de energía pico tiene un precio mucho más alto por kilovatio hora que la energía generada por las centrales eléctricas base de una red eléctrica. Una planta pico puede operar muchas horas al día, o solo unas pocas horas al año, dependiendo de la condición de la red eléctrica de la región. Debido al alto coste de la construcción de una planta de energía eficiente, si una planta pico solo va a funcionar por un tiempo corto o muy variable, no tiene sentido económico para que sea tan eficiente como una planta de energía base. Además, el equipo y los combustibles utilizados en las plantas base a menudo no son adecuados para uso en plantas de pico.\n\nLa salida de potencia de una central eléctrica pico varía dependiendo de las condiciones ambientales, por lo que el problema de negocio a resolver se podría describir como _predecir la salida de potencia de una central eléctrica pico en función de la condiciones ambientales_  - ya que esto permitiría al operador de la red hacer compensaciones económicas sobre el número de plantas pico que ha de conectar en cada momento (o si por el contrario le interesa comprar energía más cara de otra red).\n\nUna vez descrita esta lógica de negocio, primero debemos proceder a realizar un análisis exploratorio previo y trasladar el problema de negocio (predecir la potencia de salida en función de las condiciones medio ambientales) en un tarea de aprendizaje automático (ML). Por ejemplo, una tarea de ML que podríamos aplicar a este problema es la regresión, ya que tenemos un variable objetivo (dependiente) que es numérica. Para esto usaremos [Apache Spark ML Pipeline](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark-ml-package) para calcular dicha regresión.\n\nLos datos del mundo real que usaremos en esta práctica se componen de 9.568 puntos de datos, cada uno con 4 atributos ambientales recogidos en una Central de Ciclo Combinado de más de 6 años (2006-2011), proporcionado por la Universidad de California, Irvine en [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)). Para más detalles sobre el conjunto de datos visitar la página de la UCI, o las siguientes referencias:\n\n* Pinar Tufekci, [Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods](http://www.journals.elsevier.com/international-journal-of-electrical-power-and-energy-systems/), International Journal of Electrical Power & Energy Systems, Volume 60, September 2014, Pages 126-140, ISSN 0142-0615.\n* Heysem Kaya, Pinar Tufekci and Fikret S. Gurgen: [Local and Global Learning Methods for Predicting Power of a Combined Gas & Steam Turbine](http://www.cmpe.boun.edu.tr/~kaya/kaya2012gasturbine.pdf), Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering ICETCEE 2012, pp. 13-18 (Mar. 2012, Dubai)."],"metadata":{}},{"cell_type":"markdown","source":["**Tarea a realizar durante la primera parte:**\n\nRevisar la documentacion y referencias de:\n* [Spark Machine Learning Pipeline](https://spark.apache.org/docs/latest/ml-guide.html#main-concepts-in-pipelines).\n* [Databricks File System](https://docs.databricks.com/user-guide/dbutils.html)."],"metadata":{}},{"cell_type":"markdown","source":["## Parte 2: Extracción, transformación y carga [ETL] del dataset\n\n\nAhora que entendemos lo que estamos tratando de hacer, el primer paso consiste en cargar los datos en un formato que podemos consultar y utilizar fácilmente. Esto se conoce como ETL o \"extracción, transformación y carga\". Primero, vamos a cargar nuestro archivo de Amazon S3.\n\nNota: Como alternativa podemos subir nuestros datos utilizando \"Databricks Menu> Tablas> Crear tabla\", suponiendo que tengamos los archivos sin procesar en nuestro ordenador local.\n\nNuestros datos están disponibles en Amazon S3 en la siguiente ruta:\n\n```\ndbfs:/databricks-datasets/power-plant/data\n```"],"metadata":{}},{"cell_type":"markdown","source":["==========================================================================================================================================================================================================\n### Ejercicio 1 \n\nEmpezaremos por visualizar una muestra de los datos. Para esto usaremos las funciones pre-definidas en los notebooks de Databricks para explorar su sistema de archivos. Estas utilidades son las llamadas `dbutils` para trabajar en el llamado Databricks File System. La función `dbutils.fs.ls` permite listar los contenidos de un directorio. \n\n`dbutils.fs` dispone de su propio help, esta ayuda nos será de gran utilidad cuando deseemos ver las diferentes funciones disponibles."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.help()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /><h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><br /></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Usar la función `display` y la función `dbutils.fs.ls` de Databricks para listar los ficheros del directorio S3 en el que se encuentran nuestros datos que usaremos en esta PEC."],"metadata":{}},{"cell_type":"code","source":["#TODO: use display to list all the files of the directory containing the data\ndisplay(dbutils.fs.ls(\"dbfs:/databricks-datasets/power-plant/data\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/power-plant/data/Sheet1.tsv</td><td>Sheet1.tsv</td><td>308693</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/data/Sheet2.tsv</td><td>Sheet2.tsv</td><td>308693</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/data/Sheet3.tsv</td><td>Sheet3.tsv</td><td>308693</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/data/Sheet4.tsv</td><td>Sheet4.tsv</td><td>308693</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/data/Sheet5.tsv</td><td>Sheet5.tsv</td><td>308693</td></tr></tbody></table></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["Ahora, usaremos el comando `dbutils.fs.head` y la función `print` para ver los primeros 65,536 bytes del primer archivo del directorio: `Sheet1.tsv`"],"metadata":{}},{"cell_type":"code","source":["#TODO: print the first 65,536 bytes of the file Sheet1.tsv\nprint <FILL_IN>"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["==========================================================================================================================================================================================================\n### Ejercicio 2(a)\n\nAhora usaremos PySpark para visualizar las 5 primeras líneas de los datos\n\n*Hint*: Primero crea un RDD a partir de los datos usando [`sc.textFile(\"dbfs:/databricks-datasets/power-plant/data\")`](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.SparkContext.textFile).\n\n*Hint*: Luego piensa como usar el RDD creado para mostrar datos, el método [`take()`](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD.take) puede ser una buena opción a considerar."],"metadata":{}},{"cell_type":"code","source":["# TODO: Load the data and print the first five lines.\nrawTextRdd = <FILL_IN>"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["A partir nuestra exploración inicial de una muestra de los datos, podemos hacer varias observaciones sobre el proceso de ETL:\n- Los datos son un conjunto de .tsv (archivos con valores separados Tab) (es decir, cada fila de datos se separa mediante tabuladores)\n- Hay una fila de cabecera, que es el nombre de las columnas\n- Parece que el tipo de los datos en cada columna es constante (es decir, cada columna es de tipo double)\n\nEl esquema de datos que hemos obtenido de UCI es:\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vacuum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output.  Esta es la variable dependiente que queremos predecir usando los otras cuatro\n\nPara usar el paquete Spark CSV [spark-csv](https://spark-packages.org/package/databricks/spark-csv), usaremos el método [sqlContext.read.format()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.format) para especificar el formato de la fuente de datos de entrada: `'com.databricks.spark.csv'`\n\nPodemos especificar diferentes opciones de como importar los datos usando el método [options()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.options). Encontramos las opciones disponible en la documentación de GitHub del paquete [aquí](https://github.com/databricks/spark-csv#features).\n\nUsaremos las siguientes opciones:\n- `delimiter='\\t'` porque nuestros datos se encuentran delimitados por tabulaciones\n- `header='true'` porque nuestro dataset tiene una fila que representa la cabecera de los datos\n- `inferschema='true'` porque creemos que todos los datos son números reales, por lo tanto la librería puede inferir el tipo de cada columna de forma automática.\n\nEl ultimo componente necesario para crear un DataFrame es determinar la ubicación de los datos usando el método [load()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.load): `\"/databricks-datasets/power-plant/data\"`\n\nJuntando todo, usaremos la siguiente operación:\n\n`sqlContext.read.format().options().load()`"],"metadata":{}},{"cell_type":"markdown","source":["### Ejercicio 2(b)\n\nCrear un DataFrame a partir de los datos.\n- El formato es csv\n\nEn el campo opciones incluiremos 3, formadas por nombre de opción y valor, separadas por coma.\n- El separador es el tabulador\n- El fichero contiene cabecera 'header'\n- Para crear un dataframe necesitamos un esquema (schema). A partir de los datos Spark puede tratar de inferir el esquema, le diremos 'true'.\n\nNOTA: [Ayuda] https://docs.databricks.com/spark/latest/data-sources/read-csv.html\n\nEl directorio a cargar es el especificado anteriormente. Es importante indicarle a Spark que es una ubicación ya montada en el sistema dbfs, como se ha mostrado en el ejercicio 2a."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\npowerPlantDF = sqlContext.read.format(<FILL_IN>).options(<FILL_IN>).load(<FILL_IN>)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# TEST\nfrom databricks_test_helper import *\nexpected = set([(s, 'double') for s in ('AP', 'AT', 'PE', 'RH', 'V')])\nTest.assertEquals(expected, set(powerPlantDF.dtypes), \"Incorrect schema for powerPlantDF\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Vamos a comprobar los tipos de las columnas usando el metodo [dtypes](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dtypes)."],"metadata":{}},{"cell_type":"code","source":["print powerPlantDF.dtypes"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Tambien podemos examinar los datos usando el metodo `display()`."],"metadata":{}},{"cell_type":"code","source":["display(powerPlantDF)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Ahora en lugar de usar [spark-csv](https://spark-packages.org/package/databricks/spark-csv) para inferir los tipos de las columnas, especificaremos el esquema como [DataType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DataType), el cual es una lista de [StructField](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType).\n\nLa lista completa de tipos se encuentra en el modulo [pyspark.sql.types](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types). Para nuestros datos, usaremos [DoubleType()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType).\n\nPor ejemplo, para especificar cual es el nombre de la columna usaremos: `StructField(`_name_`,` _type_`, True)`. (El tercer parámetro, `True`, significa que permitimos que la columna tenga valores null.)\n\n### Ejercicio 2(c)\n\nCrea un esquema a medida para el dataset."],"metadata":{}},{"cell_type":"code","source":["# TO DO: Fill in the custom schema.\nfrom pyspark.sql.types import *\n\n# Custom Schema for Power Plant\ncustomSchema = StructType([ \\\n    <FILL_IN>, \\\n    <FILL_IN>, \\\n    <FILL_IN>, \\\n    <FILL_IN>, \\\n    <FILL_IN> \\\n                          ])"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# TEST\nTest.assertEquals(set([f.name for f in customSchema.fields]), set(['AT', 'V', 'AP', 'RH', 'PE']), 'Incorrect column names in schema.')\nTest.assertEquals(set([f.dataType for f in customSchema.fields]), set([DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType()]), 'Incorrect column types in schema.')"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Exercicio 2(d)\n\nAhora, usaremos el esquema que acabamos de crear para leer los datos. Para realizar esta operación, modificaremos el paso anterior `sqlContext.read.format`. Podemos especificar el esquema haciendo:\n- Anadir `schema = customSchema` al método load (simplemente anadelo usando una coma justo después del nombre del archivo)\n- Eliminado la opción `inferschema='true'` ya que ahora especificamos el esquema que han de seguir los datos"],"metadata":{}},{"cell_type":"code","source":["# TODO: Use the schema you created above to load the data again.\naltPowerPlantDF = sqlContext.read.format(<FILL_IN>).options(<FILL_IN>).load(<FILL_IN>)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# TEST\nfrom databricks_test_helper import *\nexpected = set([(s, 'double') for s in ('AP', 'AT', 'PE', 'RH', 'V')])\nTest.assertEquals(expected, set(altPowerPlantDF.dtypes), \"Incorrect schema for powerPlantDF\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Es importante darse cuenta que esta vez no se ha ejecutado ningún job de Spark. Esto se debe a que hemos especificado el esquema, por tanto el paquete [spark-csv](https://spark-packages.org/package/databricks/spark-csv) no tiene por qué leer los datos para inferir el esquema. Podemos usar el método [dtypes](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dtypes) para examinar el nombre y el tipo de los atributos del dataset. Estos deberían ser idénticos a los que hemos inferido anteriormente de los datos.\n\nCuando ejecutes la siguiente celda, los datos no deberían leerse."],"metadata":{}},{"cell_type":"code","source":["print altPowerPlantDF.dtypes"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Ahora podemos examinar los datos utilizando el método display(). * Ten en cuenta que esta operación hará que los datos que se lean y se creara el DataFrame. *"],"metadata":{}},{"cell_type":"code","source":["display(altPowerPlantDF)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["## Parte 3: Explorar tus Datos\n\nAhora que ya hemos cargado los datos, el siguiente paso es explorarlos y realizar algunos análisis y visualizaciones básicas.\n\nEste es un paso que siempre se debe realizar **antes de** intentar ajustar un modelo a los datos, ya que este paso muchas veces nos permitirá conocer una gran información sobre los datos."],"metadata":{}},{"cell_type":"markdown","source":["En primer lugar vamos a registrar nuestro DataFrame como una tabla de SQL llamado `power_plant`. Debido a que es posible que repitas esta práctica varias veces, vamos a tomar la precaución de eliminar cualquier tabla existente en primer lugar.\n\nPodemos eliminar cualquier tabla SQL existente `power_plant` usando el comando SQL:` DROP TABLE IF EXISTS power_plant` (también debemos que eliminar todos los ficheros asociados a la tabla, lo que podemos hacer con una operación de sistema de archivos Databricks).\n\nUna vez ejecutado el paso anterior, podemos registrar nuestro DataFrame como una tabla de SQL usando [sqlContext.registerDataFrameAsTable()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.registerDataFrameAsTable)."],"metadata":{}},{"cell_type":"markdown","source":["Ejecutar la siguiente celda con el código ya preparado."],"metadata":{}},{"cell_type":"code","source":["sqlContext.sql(\"DROP TABLE IF EXISTS power_plant\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/power_plant\", True)\nsqlContext.registerDataFrameAsTable(powerPlantDF, \"power_plant\")"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Ahora que nuestro DataFrame existe como una tabla SQL, podemos explorarlo utilizando comandos SQL.\n\nPara ejecutar SQL en una celda, utilizamos el operador `%sql`. La celda siguiente es un ejemplo del uso de SQL para consultar las filas de la tabla de SQL.\n\n**NOTE**: `%sql` es una sentencia que solo funciona en los notebooks de Databricksis. Este ejecuta `sqlContext.sql()` y pasa los resultados a la función `display()`. Estas dos sentencias son equivalentes:\n\n`%sql SELECT * FROM power_plant`\n\n`display(sqlContext.sql(\"SELECT * FROM power_plant\"))`"],"metadata":{}},{"cell_type":"markdown","source":["Ejecutar la siguiente celda con el código ya preparado."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- We can use %sql to query the rows\nSELECT * FROM power_plant"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Usa el comando de SQL `desc` para describir el esquema ejecutando la siguiente celda."],"metadata":{}},{"cell_type":"code","source":["%sql\ndesc power_plant"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["**Definición de Esquema**\n\nUna vez más, nuestro esquema es el siguiente:\n\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vacuum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output\n\nPE es nuestra variable objetivo. Este es el valor que intentamos predecir usando las otras mediciones.\n\n*Referencia [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)*"],"metadata":{}},{"cell_type":"markdown","source":["Podemos obtener el DataFrame asociado a una tabla SQL usando el método [sqlContext.table()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.table) pasando como argumento el nombre de la tabla SQL."],"metadata":{}},{"cell_type":"markdown","source":["==========================================================================================================================================================================================================\n### Ejercicio 3"],"metadata":{}},{"cell_type":"markdown","source":["Ahora vamos a realizar un análisis estadístico básico de todas las columnas.\n\nCalculad y mostrad los resultados en modo tabla (la función `display` os puede ser de ayuda):\n* Número de registros en nuestros datos\n* Media de cada columna\n* Máximo y mínimo de cada columna\n* Desviación estándar de cada columna\n\nHint: Revisad [DataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) ya que contiene métodos que permiten realizar dichos cálculos de manera sencilla."],"metadata":{}},{"cell_type":"code","source":["# Visualización de datos estadísticos básicos de nuestro dataset\ndf = <FILL_IN>"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["## <font color=\"blue\">Parte 4: Visualizar los datos</font>\n\nPara entender nuestros datos, intentamos buscar correlaciones entre las diferentes características y sus correspondientes etiquetas. Esto puede ser importante cuando seleccionamos un modelo. Por ejemplo, si una etiqueta y sus características se correlacionan de forma lineal, un modelo de regresión lineal obtendrá un buen rendimiento; por el contrario si la relación es no lineal, modelos más complejos, como arboles de decisión pueden ser una mejor opción. Podemos utilizar herramientas de visualización para observar cada uno de los posibles predictores en relación con la etiqueta como un gráfico de dispersión para ver la correlación entre ellos.\n\n==========================================================================================================================================================================================================\n### Ejercicio 4(a)\n\n** Añade las siguientes figuras: **\nVamos a ver si hay una correlación entre la temperatura y la potencia de salida. Podemos utilizar una consulta SQL para crear una nueva tabla que contenga solo el de temperatura (AT) y potencia (PE), y luego usar un gráfico de dispersión con la temperatura en el eje X y la potencia en el eje Y para visualizar la relación (si la hay) entre la temperatura y la energía.\n\nRealiza los siguientes pasos:\n\n- Ejecuta la siguiente celda\n- Haz clic en el menú desplegable junto al icono de \"Bar Chart\" y selecciona \"Scatter\" para convertir la tabla en un gráfico de dispersión\n- Haz click en \"Plot Options...\"\n- En la caja de valores, haz clic en \"Temperature\" y arrástralo antes de \"Power\"\n- Aplicar los cambios haciendo clic en el botón \"Apply\"\n- Aumentar el tamaño del grafico haciendo clic y arrastrando el control del tamaño"],"metadata":{}},{"cell_type":"code","source":["%sql\nselect <FILL_IN> from power_plant"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["Parece que hay una gran correlación entre temperatura y power output. Esta correlación es esperable gracias a la segunda ley de la termodinamica [thermal efficiency](https://en.wikipedia.org/wiki/Thermal_efficiency). Ir más allá en este análisis queda fuera del ámbito de esta práctica."],"metadata":{}},{"cell_type":"markdown","source":["### Ejercicio 4(b)\n\nUsa una sentencia SQL para crear un gráfico de dispersión entre las variables Power (PE) y Exhaust Vacuum Speed (V)."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TO DO: Replace <FILL_IN> with the appropriate SQL command.\n<FILL_IN>"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["Ahora vamos a repetir este ejercicio con el resto de variables y la etiqueta Power Output.\n\n### Ejercicio 4(c)\n\nUsa una sentencia SQL para crear un gráfico de dispersión entre las variables Power (PE) y Pressure (AP)."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TO DO: Replace <FILL_IN> with the appropriate SQL command.\n<FILL_IN>"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["### Ejercicio 4(d)\n\nUsa una sentencia SQL para crear un gráfico de dispersión entre las variables Power (PE) y Humidity (RH)."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TO DO: Replace <FILL_IN> with the appropriate SQL command.\n<FILL_IN>"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["##Parte 5: Preparación de los datos\n\nEl siguiente paso es preparar los datos para aplicar la regresión. Dado que todo el dataset es numérico y consistente, esta será una tarea sencilla y directa.\n\nEl objetivo es utilizar el método de regresión para determinar una función que nos de la potencia de salida como una función de un conjunto de características de predicción. El primer paso en la construcción de nuestra regresión es convertir las características de predicción de nuestro DataFrame a un vector de características utilizando el método [pyspark.ml.feature.VectorAssembler()](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler).\n\nEl VectorAssembler es una transformación que combina una lista dada de columnas en un único vector. Esta transformación es muy útil cuando queremos combinar características en crudo de los datos con otras generadas al aplicar diferentes funciones sobre los datos en un único vector de características. Para integrar en un único vector toda esta información antes de ejecutar un algoritmo de aprendizaje automático, el VectorAssembler toma una lista con los nombres de las columnas de entrada (lista de strings) y el nombre de la columna de salida (string)."],"metadata":{}},{"cell_type":"markdown","source":["==========================================================================================================================================================================================================\n### Ejercicio 5\n\n- Leer la documentación y los ejemplos de uso de [VectorAssembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler)\n- Convertir la tabla SQL `power_plant` en un `dataset` llamado datasetDF\n- Establecer las columnas de entrada del VectorAssember: `[\"AT\", \"V\", \"AP\", \"RH\"]`\n- Establecer la columnas de salida como `\"features\"`"],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code\nfrom pyspark.ml.feature import VectorAssembler\n\ndatasetDF = <FILL_IN>\n\nvectorizer = VectorAssembler()\nvectorizer.setInputCols(<FILL_IN>)\nvectorizer.setOutputCol(<FILL_IN>)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["# TEST\nTest.assertEquals(set(vectorizer.getInputCols()), {\"AT\", \"V\", \"AP\", \"RH\"}, \"Incorrect vectorizer input columns\")\nTest.assertEquals(vectorizer.getOutputCol(), \"features\", \"Incorrect vectorizer output column\")"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["##Parte 6: Modelar los datos\n\nAhora vamos a modelar nuestros datos para predecir que potencia de salida se dara cuando tenemos una serie de lecturas de los sensores\n\nLa API de [Apache Spark MLlib](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml) ofrece diferentes implementaciones de técnicas de regresion para modelar datasets. En este caso usaremos una muy popular, el llamado [Random Forest](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor). Se basa en la combinación de varios [Arboles de Decisión](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor), otra técnica de regresión. En este caso, Random Forest combina una cantidad grande de árboles de decisión independientes en la fase de entrenamiento probados sobre conjuntos de datos aleatorios con igual distribución.\n El objetivo es utilizarse para adaptar un modelo predictivo a un conjunto de datos observados \\\\(y\\\\) y \\\\(X\\\\). Despues de desarrollar un modelo de este tipo, dado un cierto valor  \\\\( X\\\\) del que no conocemos su valor de \\\\(y \\\\), el modelo ajustado se puede utilizarse para hacer una prediccion del valor del posible valor \\\\(y \\\\). En este caso, queremos predecir la potencia de salida.\n\nNOTA: Animamos a los alumnos a explorar las diferentes técnicas de regresión disponibles en la [API ML de Spark](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.classification)\n\nNecesitamos una forma de evaluar como de bien nuestro modelo predice la produccion de potencia en funcion de parametros de entrada. Podemos hacer esto mediante la division de nuestros datos iniciales establecidos en un _Training set_ utilizado para entrenar a nuestro modelo y un _Test set_ utilizado para evaluar el rendimiento de nuestro modelo. Podemos usar el metodo nativo de los DataFrames [randomSplit()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) para dividir nuestro dataset. El método toma una lista de pesos y una semilla aleatoria opcional. La semilla se utiliza para inicializar el generador de numeros aleatorios utilizado por la funcion de division."],"metadata":{}},{"cell_type":"markdown","source":["==========================================================================================================================================================================================================\n### Ejercicio 6(a)\n\nUtiliza el método [randomSplit()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) para dividir `datasetDF` en trainingSetDF (80% del DataFrame de entrada) y testSetDF (20% del DataFrame de entrada), para poder reproducir siempre el mismo resultado, usar la semilla 1800009193L. Finalmente, cachea (cache()) cada datafrane en memoria para maximizar el rendimiento."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL_IN> with the appropriate code.\n# We'll hold out 20% of our data for testing and leave 80% for training\nseed = 1800009193L\n(split20DF, split80DF) = datasetDF.<FILL_IN>\n\n# Let's cache these datasets for performance\ntestSetDF = <FILL_IN>\ntrainingSetDF = <FILL_IN>"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["# TEST\nTest.assertEquals(trainingSetDF.count(), 38243, \"Incorrect size for training data set\")\nTest.assertEquals(testSetDF.count(), 9597, \"Incorrect size for test data set\")"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["A continuacion vamos a crear nuestro modelo y utilizar su ayda para entender como entrenarlo. Ver la API de [Random Forest Regression](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor) para mas detalles.\n\nEjecuta la siguiente celda:"],"metadata":{}},{"cell_type":"code","source":["# ************ Decision Tree Regression ********************\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ndt = DecisionTreeRegressor()\n\nprint(dt.explainParams())\n"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["La siguiente celda esta basada en [Spark ML Pipeline API for Random Forest Regression](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor).\n\nAhora, crearemos el [ML Pipeline](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline) (flujo de ejecución) y estableceremos las fases del pipeline como vectorizar y posteriormente aplicar el regresor que hemos definido.\n\nFinalmente, crearemos el modelo entrenándolo con el DataFrame `trainingSetDF`.\n\n### Ejercicio 6(b)\n\nEl primer paso es establecer los valores de los parametros:\n- Define el nombre de la columna a donde guardaremos la prediccion como \"Predicted_PE\"\n- Define el nombre de la columna que contiene la etiqueta como \"PE\"\n- Definimos el numero de arboles de decisión utilizados\n- Definimos el grado de profundidad de la estructura en árbol"],"metadata":{}},{"cell_type":"code","source":["## TODO: Replace <FILL_IN> with the appropriate code\n# Now we set the parameters for the method\nrf.setPredictionCol(<FILL IN>)\\\n  .setLabelCol(<FILL IN>)\\\n  .setMaxDepth(5)\\\n  .setNumTrees(20)\n  \n# Chain indexer and forest in a Pipeline\npipeline = Pipeline(stages=[vectorizer, rf])\n\n# Train model.  This also runs the indexer.\nrfmodel = pipeline.fit(trainingSetDF)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["Ejecuta la celda siguiente para ver detalles acerca del modelo construido."],"metadata":{}},{"cell_type":"code","source":["# DISPLAY DETALLES DEL ARBOL CREADO\n\nprint(\"Nodos: \" + str(model.stages[-1]._java_obj.parent().getNumTrees()))\nprint(\"Profundidad: \"+ str(model.stages[-1]._java_obj.parent().getMaxDepth()))  # summary only\n\nprint(model.stages[-1]._java_obj.toDebugString())  # summary only"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["Ahora estudiaremos cómo se comportan nuestras predicciones en este modelo. Aplicamos nuestro modelo de regresión lineal para el 20% de los datos que hemos separado del conjunto de datos de entrada. La salida del modelo será una columna de producción de electricidad teórica llamada \"Predicted_PE\".\n\n- Ejecuta la siguiente celda\n- Desplázate por la tabla de resultados y observa como los valores de la columna de salida de corriente (PE) se comparan con los valores correspondientes en la salida de potencia predicha  (Predicted_PE)"],"metadata":{}},{"cell_type":"code","source":["# Apply our RF model to the test data and predict power output\n# Make predictions.\npredictions = model.transform(testSetDF)\ndisplay(predictions)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["A partir de una inspección visual de las predicciones, podemos ver que están cerca de los valores reales.\n\nSin embargo, nos gustaría disponer de una medida científica exacta de la bondad del modelo. Para realizar esta medición, podemos utilizar una métrica de evaluación como la [Error cuadrático medio](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) para validar nuestro modelo.\n\nRSME se define como: \\\\( RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}\\\\) donde \\\\(y_i\\\\) es el valor observado \\\\(x_i\\\\) es el valor predicho\n\nRMSE es una medida muy habitual para calcular las diferencias entre los valores predichos por un modelo o un estimador y los valores realmente observados. Cuanto menor sea el RMSE, mejor será nuestro modelo.\n\nSpark ML Pipeline proporciona diferentes métricas para evaluar modelos de regresión, incluyendo [RegressionEvaluator()](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator).\n\nDespués de crear una instancia de [RegressionEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator), fijaremos el nombre de la columna objetivo \"PE\" y  el nombre de la columna de predicción a \"Predicted_PE\". A continuación, invocaremos el evaluador en las predicciones.\n\n### Ejercicio 6(c)\nEjecuta la celda siguiente y asegúrate que entiendes lo que sucede."],"metadata":{}},{"cell_type":"code","source":["# Now let's compute an evaluation metric for our test dataset\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RegressionEvaluator(predictionCol=\"Predicted_PE\", labelCol=\"PE\", metricName=\"rmse\")\n\n# Run the evaluator on the DataFrame\nrmse = regEval.evaluate(predictions)\n\nprint(\"Root Mean Squared Error: %.2f\" % rmse)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["Que resultado de RMSE hemos obtenido?"],"metadata":{}},{"cell_type":"markdown","source":["Otra medida de evaluación estadística muy útil es el coeficiente de determinación, que se denota \\\\(R ^ 2 \\\\) o \\\\(r ^ 2\\\\) y pronunciado \"R cuadrado\". Es un número que indica la proporción de la variación en la variable dependiente que es predecible a partir de las variables independientes y proporciona una medida de lo bien que los resultados observados son replicados por el modelo, basado en la proporción de la variación total de los resultados explicada por el modelo. El coeficiente de determinación va de 0 a 1 (más cerca de 1), y cuanto mayor sea el valor, mejor es nuestro modelo.\n\n\nPara calcular \\\\(r^2\\\\), hemos de ejecutar el evaluador `regEval.metricName: \"r2\"`\n\nVamos a calcularlo ejecutando la celda siguiente."],"metadata":{}},{"cell_type":"code","source":["# Now let's compute another evaluation metric for our test dataset\nr2 = regEval.evaluate(predictions, {regEval.metricName: \"r2\"})\n\nprint(\"r2: {0:.2f}\".format(r2))"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["Que resultado de \\\\(r^2\\\\) hemos obtenido? A partir de dicho parametro, crees que el modelo calculado se ajusta bien a los datos?"],"metadata":{}},{"cell_type":"markdown","source":["En general, suponiendo una distribución Gaussiana de errores, un buen modelo tendrá 68% de las predicciones dentro de 1 RMSE y 95% dentro de 2 RMSE del valor real (ver http://statweb.stanford.edu/~susan/courses/s60/split/node60.html).\n\nVamos a examinar las predicciones y ver si un RMSE como el obtenido cumple este criterio.\n\nCrearemos un nuevo DataFrame usando [selectExpr()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.selectExpr) para generar un conjunto de expresiones SQL, y registrar el DataFrame como una tabla de SQL utilizando [registerTempTable()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable).\n\n### Ejercicio 6(d)\n\nEjecuta la celda siguiente y asegúrate que entiendes lo que sucede."],"metadata":{}},{"cell_type":"code","source":["# First we remove the table if it already exists\nsqlContext.sql(\"DROP TABLE IF EXISTS Power_Plant_RMSE_Evaluation\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/Power_Plant_RMSE_Evaluation\", True)\n\n# Next we calculate the residual error and divide it by the RMSE\npredictions.selectExpr(\"PE\", \"Predicted_PE\", \"PE - Predicted_PE Residual_Error\", \"(PE - Predicted_PE) / {} Within_RSME\".format(rmse)).registerTempTable(\"Power_Plant_RMSE_Evaluation\")"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["Podemos utilizar sentencias SQL para explorar la tabla `Power_Plant_RMSE_Evaluation`. En primer lugar vamos a ver qué datos en la tabla utilizando una sentencia SELECT de SQL.\n\nCompleta la siguiente consulta para que devuelva todos los elementos de la tabla `Power_Plant_RMSE_Evaluation`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT <FILL_IN> from <FILL_IN>"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["Ahora podemos mostrar el RMSE como un histograma.\n\n### Ejercicio 6(e)\n\nEjecuta los siguientes pasos:\n\n- Ejecuta la siguiente celda\n- Haz clic en el menú desplegable junto al icono \"Bar chart\" y selecciona \"Histogram\" para convertir la tabla en un histograma\n- Haz clic en \"Plot Options...\"\n- En la caja \"All Fields:\", haz clic \"&lt;id&gt;\" y arrástralo dentro de la caja \"Keys:\"\n- Cambia el valor \"Aggregation\" a \"COUNT\"\n- Aplicar los cambios haciendo clic en el botón Aplicar\n- Aumentar el tamaño del grafico haciendo clic y arrastrando el control del tamaño\n\nObserva que el histograma muestra claramente que el RMSE se centra alrededor de 0 con la gran mayoría de errores dentro de 2 RMSE."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- Now we can display the RMSE as a Histogram\nSELECT Within_RSME  from Power_Plant_RMSE_Evaluation"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["Usando una instrucción SELECT de SQL un poco más compleja, podemos contar el número de predicciones dentro de + o - 1,0 y + o - 2,0 y luego mostrar los resultados como un gráfico circular.\n\nEjecuta los siguientes pasos:  \n  - Ejecutar la siguiente celda\n  - Haz clic en el menú desplegable junto al icono de \"Bar chart\" y selecciona \"Pie\" para convertir la tabla en un gráfico de sectores\n  - Aumentar el tamaño del grafico haciendo clic y arrastrando el control del tamaño"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT case when Within_RSME <= 1.0 AND Within_RSME >= -1.0 then 1\n            when  Within_RSME <= 2.0 AND Within_RSME >= -2.0 then 2 else 3\n       end RSME_Multiple, COUNT(*) AS count\nFROM Power_Plant_RMSE_Evaluation\nGROUP BY case when Within_RSME <= 1.0 AND Within_RSME >= -1.0 then 1  when  Within_RSME <= 2.0 AND Within_RSME >= -2.0 then 2 else 3 end"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["### Conclusiones sobre el modelo\nA partir del pie chart, responde:\n- ¿Cuantas predicciones estan a 1 RMSE como máximo de los valores reales?\n- ¿Y cuantas predicciones estan a 2 RMSE como máximo de los valores reales?"],"metadata":{}},{"cell_type":"markdown","source":["##Parte 7: Ajustar y evaluar\n\nAhora que tenemos un primer modelo bastante bueno vamos a tratar de hacer uno aun mejor ajustando sus parametros. El proceso de ajustar un modelo se conoce como [Model Selection](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.tuning) o [Hyperparameter Tuning](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.tuning). Spark ML Pipeline hace que el proceso de ajuste sea sencillo.\n\nSpark ML Pipeline soporta la seleccion de modelos usando herramientas herramientas como el [CrossValidator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator), que requiere los siguientes elementos:\n- [Estimator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Estimator): un algoritmo o un pipeline a ajustar\n- [Conjunto de ParamMaps](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder): parametros para elegir, tambien conocido como _parameter grid_\n- [Evaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator): metrica para medir que tan bien lo hace un modelo sobre los datos de entrenamiento\n\nA un alto nivel, las herramientas de seleccion de modelos, tales como [CrossValidator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) trabajan de la siguiente manera:\n\n- Se separaran los datos de entrada en dos conjuntos entrenamiento y test.\n- Para cada uno de estos pares (entrenamiento, test), hay iterar a traves del conjunto de ParamMaps:\n    - Para cada [ParamMap](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder), se ajusta el [Estimador](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Estimator) usando dichos parametros, se obtiene el modelo ajustado, y se evaluar su rendimiento usando el [Evaluator] (https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator).\n    - Seleccionan el mejor modelo producido por el conjunto de parametros.\n\nEl [Evaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator) puede ser por ejemplo un [RegressionEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator) para problemas de regresion. Como ayuda a construir el conjunto de parametros, los usuarios pueden utilizar la utilidad [ParamGridBuilder](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder).\n\nTen en cuenta que la validacion cruzada sobre una conjunto grande de parametros es costosa.\n\nEn el siguiente apartado llevaremos a cabo los siguientes pasos:\n- Crear un [CrossValidator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) utilizando un pipeline y un [RegressionEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator) que hemos creado anteriormente, y establecer el numero de pliegues (folds) a 5\n- Crear una lista de 10 valores distintos para la profundidad de nuestro 'tree'.\n  - Crear una lista de 10 parametros de numero de trees.\n- Usar [ParamGridBuilder](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) para construir un conjunto de parametros con los parametros de profundidad y numero de árboles y anadir dicho conjunto al [CrossValidator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)\n- Ejecutar el [CrossValidator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) para encontrar los parametros que producen el mejor modelo (es decir, mas bajo RMSE) y devolver el mejor modelo."],"metadata":{}},{"cell_type":"markdown","source":["##Responde las siguientes preguntas\n- ¿A partir de los resultados del ejercicio 7, cuál es la profundidad y numero de trees de nuestro modelo que ofrece mejores resultados? ¿Podríamos seguir incrementando sus valores indefinidamente? (podéis repetir el ejercicio variando estos valores en la celda correspondiente y analizando su impacto en el resultado final)\n- ¿Cuál te ha parecido la mayor desventaja que ofrece este método de regresión?"],"metadata":{}},{"cell_type":"markdown","source":["##Parte 8: Propuesta de modelo alternativo\n\nEn base a lo desarrollado en apartados anteriores, tomad una técnica de [regresión de las que Spark](https://spark.apache.org/docs/latest/ml-classification-regression.html) proporciona y aplicadla al problema (Regresión Lineal, Decision Tree, etc.)\nPodéis re-utilizar gran parte del código que ya habéis escrito para realizar el modelado del ejercicio 6 en adelante. Para dicho modelo, calcula RSME y el porcentaje de valores que caen en 1 RMSE y 2 RMSE."],"metadata":{}},{"cell_type":"markdown","source":["##Responde \n- ¿A partir de los resultados obtenidos del ejercicio 8, cual crees que en el mejor modelo para el conjunto de datos (ten en cuenta factores como complejidad, rendimiento, ajuste, etc.)?"],"metadata":{}}],"metadata":{"name":"Lab_2_ML_pipeline","notebookId":2094854598082171},"nbformat":4,"nbformat_minor":0}
