{"cells":[{"cell_type":"markdown","source":["# ![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n\n# Procesamiento por lotes (batch) vs. interactivo (streaming)\n\nApache Spark incluyó en su versión 2.0 la primera versión de una nueva API para el procesamiento en flujo de \"nivel superior\", en inglés la denominó \"Structured Streaming\". En esta PEC veremos como usar dicha API sobre Spark DataFrame para construir aplicaciones de \"Flujo Estructurado\". Nuestro objetivo será calcular métricas en tiempo real, como conteos, o promedios en tiempo real dentro de ventanas (p.ej. _moving average_) en una secuencia de acciones con marca de tiempo (p.ej. acciones Abrir y Cerrar en nuestros datos de muestra).\n\n** Esta PEC cubrirá: **\n* *Parte 1: Conocimiento del dominio*\n* *Parte 2: Procesamiento por lotes* (3 puntos sobre 10)\n* *Parte 3: Procesamiento interactivo* (7 puntos sobre 10)"],"metadata":{}},{"cell_type":"markdown","source":["#### Parte 1. Datos de esta PEC\n\nPodemos encontrar algunos ejemplos de datos en flujo en los archivos ubicados en ```/databricks-datasets/structured-stream/events/```. Estos datos son los que vamos a usar para construir las diferentes métricas. Veamos que contiene este directorio ejecutando la siguiente celda."],"metadata":{}},{"cell_type":"code","source":["%fs ls /databricks-datasets/structured-streaming/events/"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Hay aproximadamente unos 50 archivos JSON. Veamos que contiene uno de ellos, por ejemplo el archivo ```file-0.json```"],"metadata":{}},{"cell_type":"code","source":["%fs head /databricks-datasets/structured-streaming/events/file-0.json"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Cada linea del archivo contiene un registro JSON con dos campos: ```tiempo``` y ```acción```. Tratemos de analizar estos archivos primero como si fueran ficheros en lote y luego de forma interactiva."],"metadata":{}},{"cell_type":"markdown","source":["#### Parte 2. Procesamiento por lotes\n\nEl primer paso habitual para intentar procesar los datos es consultar los mismos de forma estática. Definamos para ello un DataFrame basado en el formato de los archivos y guardemos dicho DataFrame en formato de tabla.\n\nEn esta PEC no introduciremos aún como funcionan los tipos en pySpark. Esto lo haremos durante las siguientes PEC. Igualmente, para entender que estamos haciendo en la siguiente celda podemos consultar la lista completa de tipos se encuetra en el módulo [pyspark.sql.types](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#module-pyspark.sql.types). Para nuestros datos, usaremos los tipos [TimestampType()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.TimestampType) y [StringType()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.StringType)."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ninputPath = \"/databricks-datasets/structured-streaming/events/\"\n\n# Dado que ya hemos analizado un poco los datos y conocemos su formato, definiremos el esquema para acelerar el procesamiento (no es necesario que Spark intente inferir su esquema)\njsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n\n# Static DataFrame que representa datos en los archivos JSON\nstaticInputDF = (\n  spark\n    .read\n    .schema(jsonSchema)\n    .json(inputPath)\n)\n\n# Esta instruccion se ocupa de almacenar el DataFrame como una tabla de SparkQL, así podremos accederla usando lenguaje SQL\nstaticInputDF.createOrReplaceTempView(\"static_input\")\n\ndisplay(staticInputDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Antes de empezar a trabajar con estos datos, reduciremos su granularidad temporal a nivel de minuto para cada tipo de acción. Además, generaremos una vista para poder usar consultas SQL y así poder calcular nuestras métricas de forma sencilla.\n\nPara realizar este proceso, ejecutaremos la siguiente celda."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *      # para poder usar la funcion window()\n\nstaticCountsDF = (\n  staticInputDF\n    .groupBy(\n       staticInputDF.action, \n       window(staticInputDF.time, \"1 minute\"))    \n    .count()\n)\nstaticCountsDF.cache()\n\n# Registrar el DataFrame como una tabla llamada 'static_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")\n\ndisplay(staticCountsDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Ejercicio 2(a)\n\nModifica la granularidad del dataframe ```staticCountsDF``` a nivel de hora y repite el mismo conteo. Para superar el test, la columna de salida del dataFrame ha de conservar el mismo nombre de columna que en el dataFrame ```staticCountsDF```.\n\n*Hint*: Puedes usar la opción ```.withColumnRenamed(\"original_name\", \"desired_name\")``` de la operación groupBy() para cambiar el nombre de las columnas del dataFrame."],"metadata":{}},{"cell_type":"code","source":["staticCountsHourlyDF = (\n  <FILL IN>\n)\n\n# Registrar el DataFrame como una tabla llamada 'static_mean'\nstaticCountsHourlyDF.createOrReplaceTempView(\"static_counts_hourly\")\n\ndisplay(staticCountsHourlyDF)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#Test\nfrom databricks_test_helper import *\n\nTest.assertEquals(spark.sql(\"select max(count) from static_counts_hourly\").rdd.flatMap(list).first(), 1036, \"Incorrect couting by hour\")\nTest.assertEquals(spark.sql(\"select min(count) from static_counts_hourly\").rdd.flatMap(list).first(), 11, \"Incorrect couting by hour\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Ejercicio 2(b)\n\nAhora que hemos registrado la vista ```static_counts_hourly``` usando el dataframe ```static_counts_hourly```, calcula usando una consulta SQL el número de acciones totales de cada tipo (```Open```, ```Close```).\n\n**IMPORTANTE**: Recuerda usar ```as``` para renombrar la columna con la suma como ```total_count```."],"metadata":{}},{"cell_type":"code","source":["sum_static_counts_hourly = spark.sql(<FILL IN>)\n\nsum_static_counts_hourly.show()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["Test.assertEquals(sum_static_counts_hourly.take(1)[0].asDict()['total_count'], 50000, \"Incorrect total counting\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Ejercicio 2(c)\n\nAhora vamos a complicar un poco el ejercicio. Cuenta el numero de acciones totales por minuto y tipo.\n\n**IMPORTANTE**: Recuerda usar el dataframe ```static_counts```"],"metadata":{}},{"cell_type":"code","source":["window_static_counts_minute = <FILL IN>\n\nwindow_static_counts_minute.show()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["Test.assertEquals(window_static_counts_minute.take(3)[2].asDict()['count'], 11, \"Incorrect counting\")\n\nTest.assertEquals(window_static_counts_minute.count(),6122,\"Incorrect number of minutes\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Ejercicio 2(d)\n\nAhora que ya somos capaces de contar de varias formas y con diferentes granularidades, vamos a calcular algun estadístico muy simple, como por ejemplo la media. \n\nUsando un código parecido al del _ejercicio 2(a)_, calcula el promedio de acciones por minuto para cada hora, independientemente si son acciones ```Open``` o ```Close```. \n\n**IMPORTANTE**: Recuerda renombrar la columna donde calculas la media como ```average```."],"metadata":{}},{"cell_type":"code","source":["StaticAverageHourlyDF = <FILL IN>\n\n# Registrar el DataFrame como una tabla llamada 'static_mean'\nStaticAverageHourlyDF.createOrReplaceTempView(\"Static_Average_Hourly\")\n\ndisplay(StaticAverageHourlyDF)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["Test.assertEquals(StaticAverageHourlyDF.take(2)[1].asDict()['average'], 16.575, \"Incorrect averaging\")\nTest.assertEquals(StaticAverageHourlyDF.take(3)[2].asDict()['average'], 16.725, \"Incorrect averaging\")\n\nTest.assertEquals(StaticAverageHourlyDF.count(),53,\"Incorrect number of minutes\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Ejercicio 2(e)\n\nPara concluir nuestros cálculos en batch, determina la hora en la que se han producido un mayor número de acciones promedio por minuto."],"metadata":{}},{"cell_type":"code","source":["max_static_averages_hourly = <FILL IN>"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["¿Qué hora ha sido la que ha tenido una mayor actividad promedio por minuto?"],"metadata":{}},{"cell_type":"markdown","source":["#### Parte 3: Procesamiento interactivo\n\nAhora que hemos analizado los datos de forma estática, vamos a cambiar el análisis a una consulta que se actualice continuamente a medida que llegan nuevos datos. Como solo tenemos un conjunto estático de archivos, vamos a emular un flujo leyendo un archivo a la vez, en el orden cronológico en que fueron creados. La consulta que tenemos que escribir es prácticamente la misma que la anterior."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Parecido a la definicion staticInputDF anterior, solo hemos cambiado `readStream` en lugar de `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(jsonSchema)               # Instanciamos el esquema de datos en formato JSON\n    .option(\"maxFilesPerTrigger\", 1)  # Trataremos los archivos como si fueran una secuencia, seleccionando un archivo a la vez\n    .json(inputPath)\n)\n\n# Misma consulta que en el caso staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.action, \n      window(streamingInputDF.time, \"1 minute\"))\n    .count()\n)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Vamos a comprobar que realmente disponemos de un stream de datos"],"metadata":{}},{"cell_type":"code","source":["streamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Ahora vamos a establecer la configuración en el cluster del flujo de datos."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # mantenemos pequeno el tamaño de los shuffle\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table\n    .queryName(\"counts\")     # counts = nombre de la tabala in-memory\n    .outputMode(\"complete\")  # complete = todos los contadores deben guardarse en la tabla\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Ejercicio 3(a)\n\nPor simplicidad en esta parte podemos usar la siguiente notación:\n\n`%sql` que es una sentencia que solo funciona en los notebooks de Databricks. Esta ```magic function``` ejecuta `sqlContext.sql()` y pasa los resultados a la función `display()`. Estas dos sentencias son equivalentes:\n\n`%sql select * from counts order by window`\n\n`display(sqlContext.sql(\"select * from counts  order by window\"))`\n\n**Nota:** Como el comando display se ejecuta en el navegador no en el cluster, está limitado a solo mostrar las 1000 primeras filas, para contar cuantas filas se han leído del stream podéis ejecutar el siguiente código:\n\n`%sql select count(*) from counts`"],"metadata":{}},{"cell_type":"code","source":["<FILL IN>"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["En la celda de arriba, además, puedes cambiar la forma de visualizar los datos, ex. En forma de tabla, histograma, linea, etc.\n\nVisualiza en forma de histograma los resultados y re-ejecuta unas cuantas veces la celda. Podrás observar que conforme van llegando nuevos datos, la gráfica se va actualizando."],"metadata":{}},{"cell_type":"markdown","source":["### Ejercicio 3(b)\n\nVamos a crear un sistema de alerta muy sencillo que nos indique cuando, en un minuto, hay una diferencia mayor de 20 acciones entre los contadores de las acciones ```Open``` y ```Close```.\n\n** NOTA:** Recuerda re-ejecuta las celdas de la parte 3 para re-iniciar el flujo de datos. Sólo hay tres minutos en todo el dataset donde la condición descrita anteriormente se cumple."],"metadata":{}},{"cell_type":"code","source":["from time import sleep\n\nfor i in range(10):\n  <FILL IN>\n  sleep(5)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["### Ejercicio 3(c)\n\n\nAhora vamos a calcular la [media móvil simple](https://en.wikipedia.org/wiki/Moving_average) del número de acciones de los últimos 30 minutos. Tienes los detalles de como realizar este cálculo [aquí](https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average)."],"metadata":{}},{"cell_type":"code","source":["for i in range(10):\n  <FILL IN>\n  sleep(5)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### Ejercicio 3(d)\n\nAhora vamos a calcular la [varianza](https://es.wikipedia.org/wiki/Varianza) en el número de acciones. Tienes los detalles de como calcular la varianza online [aquí](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm). Para esto, recupera usando una sentencia sql, la suma de las acciones del último minuto y ves actualizando el resultado de la varianza."],"metadata":{}},{"cell_type":"code","source":["for i in range(50):\n  <FILL IN>\n  sleep(5)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["### Ejercicio 3(d)\n\nResponde las siguientes preguntas:\n\n- ¿Qué es una arquitectura lambda? ¿Spark cumple con esta definición?\n- ¿El código que has utilizado en la parte de streaming es reusable para procesado batch? ¿y viceversa?"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":42}],"metadata":{"name":"PEC2_batch_streaming","notebookId":4265993690174312},"nbformat":4,"nbformat_minor":0}
